{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component: Model I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv     # dotenv 파일에서 환경변수 로드\n",
    "load_dotenv()   # .env 파일에서 환경변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # 환경변수 사용을 위한 모듈\n",
    "os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자동차를 홍보하기 위한 재미있고, 신선한 광고문구를 작성해 주세요\n"
     ]
    }
   ],
   "source": [
    "# ProptTemplate\n",
    "from langchain import PromptTemplate    # 프롬프트 템플릿을 사용하기 위한 모듈\n",
    "\n",
    "template = \"{product}를 홍보하기 위한 재미있고, 신선한 광고문구를 작성해 주세요\"    # 프롬프트 템플릿 정의\n",
    "\n",
    "# PromptTemplate 객체 생성\n",
    "prompt = PromptTemplate(\n",
    "    template=template,  # 프롬프트 템플릿\n",
    "    input_variables=['product'] # 입력 변수 정의\n",
    ")\n",
    "\n",
    "print(prompt.format(product='자동차'))  # 프롬프트 템플릿에 '자동차'를 넣어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 계산 문제를 해결하세요\n",
      "\n",
      "Q: 2 + 2는 무엇인가요?\n",
      "A: 2 + 2 = 4\n",
      "\n",
      "Q: 3 + 4는 무엇인가여?\n",
      "A: 3 + 4 = 7\n",
      "\n",
      "Q: 24 + 78은 무엇인가요?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# FewShotPromptTemplate\n",
    "from langchain.prompts import FewShotPromptTemplate # FewShotPromptTemplate을 사용하기 위한 라이브러리\n",
    "\n",
    "# FewShotPromptTemplate은 예시를 제공하여 모델이 더 나은 결과를 생성하도록 돕는 템플릿\n",
    "examples = [\n",
    "    {\"question\": \"2 + 2는 무엇인가요?\", \"answer\": \"2 + 2 = 4\"}, \n",
    "    {\"question\": \"3 + 4는 무엇인가여?\", \"answer\": \"3 + 4 = 7\"}\n",
    "]\n",
    "\n",
    "# 예시를 프롬프트 템플릿에 맞게 변환\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"Q: {question}\\nA: {answer}\",  # 예시 프롬프트 템플릿\n",
    "    input_varialbes=['question', 'answer']  # 입력 변수 정의\n",
    ")\n",
    "\n",
    "# 예시 프롬프트 템플릿을 사용하여 예시를 생성\n",
    "fewshot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,  # 예시 리스트\n",
    "    example_prompt=example_prompt,  # 예시 프롬프트 템플릿\n",
    "    prefix='다음 계산 문제를 해결하세요',   # 프롬프트 접두사\n",
    "    suffix=\"Q: {question}은 무엇인가요?\\nA:\",   # 프롬프트 접미사\n",
    "    input_variables=['question']    # 입력 변수 정의\n",
    ")\n",
    "\n",
    "print(fewshot_prompt.format(question='24 + 78'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='당신은 친절한 쳇봇입니다.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='질문: AI를 배우려면 무엇부터 해야 하나요?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ChatPromptTemplate = System, Human, AI 유형별 메세지 작성\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "\n",
    "sys_msg = SystemMessagePromptTemplate.from_template('당신은 친절한 쳇봇입니다.')    # 시스템 메시지 템플릿 생성\n",
    "hm_msg = HumanMessagePromptTemplate.from_template('질문: {question}')   # 사용자 메시지 템플릿 생성\n",
    "msg = ChatPromptTemplate.from_messages([sys_msg, hm_msg])   # 챗 프롬프트 템플릿 생성\n",
    "\n",
    "msg.format_messages(question='AI를 배우려면 무엇부터 해야 하나요?') # 메시지 포맷팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputparser 인스턴스 및 promptTemplate 인스턴스 생성\n",
    "from langchain_openai import ChatOpenAI   # OpenAI 챗 모델을 사용하기 위한 라이브러리\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser   # 출력 파서 라이브러리\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()   # 쉼표로 구분된 리스트 출력 파서 생성\n",
    "format_instructions = output_parser.get_format_instructions()   # 출력 포맷 지침 확인\n",
    "\n",
    "prompt_tpl = PromptTemplate(\n",
    "    template=\"{subject} 5개의 팀을 알려주세요.\\n형식 지정:{format}\",    # 프롬프트 템플릿\n",
    "    input_variables=['subject'],                                      # 사용자 입력 변수 정의\n",
    "    partial_variables={'format': format_instructions},                # 고정 설정 변수 정의\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 프로야구팀 5개의 팀을 알려주세요.\n",
      "형식 지정:Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 생성\n",
    "query = \"한국의 프로야구팀\" # 사용자 입력값\n",
    "\n",
    "prompt = prompt_tpl.format(subject=query)   # 프롬프트 템플릿에 사용자 입력값을 넣어 포맷팅\n",
    "print(prompt)   # 포맷팅된 프롬프트 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openAI api 모델 사용을 위한 인스턴스 생성 및 요청 처리\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",   # 사용할 모델 이름\n",
    "    temperature=0,   # 생성된 텍스트의 다양성 조절 (0: 결정적, 1: 무작위적)\n",
    "    max_tokens=2048,   # 생성할 최대 토큰 수\n",
    ")\n",
    "\n",
    "model.invoke(prompt)   # 모델에 프롬프트(질의)를 전달하여 응답 생성\n",
    "\n",
    "response = model.invoke(prompt)   # 모델에 프롬프트(질의)를 전달하여 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두산 베어스, LG 트윈스, 삼성 라이온즈, 키움 히어로즈, NC 다이노스\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# response   # 모델의 응답 출력\n",
    "print(response.content)   # 모델의 응답 내용 출력\n",
    "print(type(response.content))   # 응답 객체의 타입 확인 string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['두산 베어스', 'LG 트윈스', '삼성 라이온즈', '키움 히어로즈', 'NC 다이노스']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(output_parser.parse(response.content))   # 응답 내용을 파싱하여 리스트 형태로 출력\n",
    "print(type(output_parser.parse(response.content)))   # 파싱된 응답의 타입 확인 string -> list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/models/Bllossom/llama-3.2-Korean-Bllossom-3B/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/models/Bllossom/llama-3.2-Korean-Bllossom-3B/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 19\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ChatHuggingFace 인스턴스 생성\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ChatHuggingFace는 Hugging Face의 챗 모델을 사용하기 위한 클래스\u001b[39;00m\n\u001b[0;32m     14\u001b[0m hf_model \u001b[38;5;241m=\u001b[39m ChatHuggingFace(\n\u001b[0;32m     15\u001b[0m     llm\u001b[38;5;241m=\u001b[39mend_point,  \u001b[38;5;66;03m# HuggingFaceEndpoint 인스턴스를 LLM으로 사용\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m    \u001b[38;5;66;03m# 모델의 응답을 자세히 출력할지 여부 (True: 자세히, False: 간단히)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mhf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# 모델에 프롬프트(질의)를 전달하여 응답 생성\u001b[39;00m\n\u001b[0;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m hf_model\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m저는 아침으로 사과를 먹었습니다. 저는 아침에 뭘 먹었을까요?\u001b[39m\u001b[38;5;124m'\u001b[39m)   \u001b[38;5;66;03m# 모델에 프롬프트(질의)를 전달하여 응답 생성\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:368\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    364\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    365\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    369\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    370\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    371\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    372\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    373\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    374\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    375\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    376\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    377\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    378\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:937\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    936\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:759\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    758\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 759\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    760\u001b[0m                 m,\n\u001b[0;32m    761\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    762\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    763\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    764\u001b[0m             )\n\u001b[0;32m    765\u001b[0m         )\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    767\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1002\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1002\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1003\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1004\u001b[0m     )\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1006\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:370\u001b[0m, in \u001b[0;36mChatHuggingFace._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_huggingface_endpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm):\n\u001b[0;32m    369\u001b[0m     message_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m--> 370\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mchat_completion(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(answer)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:992\u001b[0m, in \u001b[0;36mInferenceClient.chat_completion\u001b[1;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[0;32m    964\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload_model,\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    984\u001b[0m }\n\u001b[0;32m    985\u001b[0m request_parameters \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39mprepare_request(\n\u001b[0;32m    986\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    987\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    990\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken,\n\u001b[0;32m    991\u001b[0m )\n\u001b[1;32m--> 992\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\vectordb_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable for url: https://router.huggingface.co/hf-inference/models/Bllossom/llama-3.2-Korean-Bllossom-3B/v1/chat/completions"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint   # HuggingFace 모델을 사용하기 위한 라이브러리\n",
    "from langchain_huggingface import ChatHuggingFace   # HuggingFace 챗 모델을 사용하기 위한 라이브러리\n",
    "\n",
    "# HuggingFaceEndpoint 인스턴스 생성\n",
    "# HuggingFaceEndpoint는 Hugging Face의 모델을 API로 호출하기 위한 클래스\n",
    "end_point = HuggingFaceEndpoint(\n",
    "    repo_id=\"Bllossom/llama-3.2-Korean-Bllossom-3B\",    # 사용할 모델의 레포지토리 ID\n",
    "    task=\"text-generation\", # 모델의 작업 유형 (text-generation: 텍스트 생성)\n",
    "    max_new_tokens=1024,    # 생성할 최대 토큰 수\n",
    ")\n",
    "\n",
    "# ChatHuggingFace 인스턴스 생성\n",
    "# ChatHuggingFace는 Hugging Face의 챗 모델을 사용하기 위한 클래스\n",
    "hf_model = ChatHuggingFace(\n",
    "    llm=end_point,  # HuggingFaceEndpoint 인스턴스를 LLM으로 사용\n",
    "    verbose=True    # 모델의 응답을 자세히 출력할지 여부 (True: 자세히, False: 간단히)\n",
    ")\n",
    "\n",
    "hf_model.invoke(prompt)   # 모델에 프롬프트(질의)를 전달하여 응답 생성\n",
    "response = hf_model.invoke('저는 아침으로 사과를 먹었습니다. 저는 아침에 뭘 먹었을까요?')   # 모델에 프롬프트(질의)를 전달하여 응답 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.modlel_laberathy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodlel_laberathy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelLaboratory     \u001b[38;5;66;03m# 모델 실험을 위한 라이브러리\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model_lab \u001b[38;5;241m=\u001b[39m ModelLaboratory\u001b[38;5;241m.\u001b[39mfrom_llms([model, hf_model])   \u001b[38;5;66;03m# 모델 실험실 인스턴스 생성\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model_lab\u001b[38;5;241m.\u001b[39mcompare(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m아침에 사과를 먹는 것의 효과를 알려주세요.\u001b[39m\u001b[38;5;124m'\u001b[39m)   \u001b[38;5;66;03m# 모델 실험실 인스턴스 생성\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain.modlel_laberathy'"
     ]
    }
   ],
   "source": [
    "from langchain.modlel_laberathy import ModelLaboratory     # 모델 실험을 위한 라이브러리\n",
    "\n",
    "model_lab = ModelLaboratory.from_llms([model, hf_model])   # 모델 실험실 인스턴스 생성\n",
    "model_lab.compare('아침에 사과를 먹는 것의 효과를 알려주세요.')   # 모델 실험실 인스턴스 생성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
