{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb357ad",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863f73b",
   "metadata": {},
   "source": [
    "- **Word Embedding**은 단어를 고정된 차원의 벡터로 변환하는 기술로, 단어 간의 의미적 유사성을 반영하도록 학습된 벡터를 말한다.\n",
    "- 이 기술은 자연어 처리에서 문장을 처리하고 이해하는 데 활용된다.\n",
    "- 숫자로 표현된 단어 목록을 통해 감정을 추출하는 것도 가능하다.\n",
    "- 연관성 있는 단어들을 군집화하여 다차원 공간에 벡터로 나타낼 수 있으며, 이는 단어나 문장을 벡터 공간에 매핑하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00aa3f3",
   "metadata": {},
   "source": [
    "**Embedding Matrix 예시**\n",
    "\n",
    "*아래 표의 벡터 값들은 모두 기계 학습을 통해 학습된 결과이다.*  \n",
    "\n",
    "| Dimension | Man (5391) | Woman (9853) | King (4914) | Queen (7157) | Apple (456) | Orange (6257) |\n",
    "|-----------|------------|--------------|-------------|--------------|-------------|---------------|\n",
    "| 성별      | -1         | 1            | -0.95       | 0.97         | 0.00        | 0.01          |\n",
    "| 귀족      | 0.01       | 0.02         | 0.93        | 0.95         | -0.01       | 0.00          |\n",
    "| 나이      | 0.03       | 0.02         | 0.7         | 0.69         | 0.03        | -0.02         |\n",
    "| 음식      | 0.04       | 0.01         | 0.02        | 0.01         | 0.95        | 0.97          |\n",
    "\n",
    "<br>\n",
    "\n",
    "*아래는 전치된 표이다.*\n",
    "\n",
    "| Word          | 성별   | 귀족   | 나이   | 음식   |\n",
    "|---------------|--------|--------|--------|--------|\n",
    "| Man (5391)    | -1.00  | 0.01   | 0.03   | 0.04   |\n",
    "| Woman (9853)  | 1.00   | 0.02   | 0.02   | 0.01   |\n",
    "| King (4914)   | -0.95  | 0.93   | 0.70   | 0.02   |\n",
    "| Queen (7157)  | 0.97   | 0.95   | 0.69   | 0.01   |\n",
    "| Apple (456)   | 0.00   | -0.01  | 0.03   | 0.95   |\n",
    "| Orange (6257) | 0.01   | 0.00   | -0.02  | 0.97   |\n",
    "\n",
    "- **의미적 유사성 반영**  \n",
    "  - 단어를 고정된 크기의 실수 벡터로 표현하며, 비슷한 의미를 가진 단어는 벡터 공간에서 가깝게 위치한다.  \n",
    "  - 예를 들어, \"king\"과 \"queen\"은 비슷한 맥락에서 자주 사용되므로 벡터 공간에서 가까운 위치에 배치된다.  \n",
    "\n",
    "- **밀집 벡터(Dense Vector)**  \n",
    "  - BoW, DTM, TF-IDF와 달리 Word Embedding은 저차원 밀집 벡터로 변환되며, 차원이 낮으면서도 의미적으로 풍부한 정보를 담는다.  \n",
    "  - 벡터 차원은 보통 100 또는 300 정도로 제한된다.  \n",
    "\n",
    "- **문맥 정보 반영**  \n",
    "  - Word Embedding은 단어 주변의 단어들을 학습해 단어의 의미를 추론한다.  \n",
    "  - 예를 들어, \"bank\"라는 단어가 \"river\"와 함께 나오면 \"강둑\"을, \"money\"와 함께 나오면 \"은행\"을 의미한다고 학습한다.  \n",
    "\n",
    "- **학습 기반 벡터**  \n",
    "  - Word Embedding은 대규모 텍스트 데이터에서 단어 간 연관성을 학습해 벡터를 생성한다.  \n",
    "  - 반면, BoW나 TF-IDF는 단순한 규칙 기반 벡터화 방법이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84814ed",
   "metadata": {},
   "source": [
    "### 희소 표현(Sparse Representation) | 분산 표현(Distributed Representation)\n",
    "- 원-핫 인코딩으로 얻은 원-핫 벡터는 단어의 인덱스 값만 1이고 나머지는 모두 0으로 표현된다.\n",
    "- 이렇게 대부분의 값이 0인 벡터나 행렬을 사용하는 표현 방식을 희소 표현(sparse representation)이라고 한다.  \n",
    "- 희소 표현은 단어 벡터 간 유의미한 유사성을 표현할 수 없다는 단점이 있다.\n",
    "- 이를 해결하기 위해 단어의 의미를 다차원 공간에 벡터화하는 분산 표현(distributed representation)을 사용한다.\n",
    "- 분산 표현으로 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라고 하며, 이렇게 변환된 벡터를 임베딩 벡터(embedding vector)라고 한다.  \n",
    "- **원-핫 인코딩 → 희소 표현**  \n",
    "- **워드 임베딩 → 분산 표현**  \n",
    "\n",
    "**분산 표현(Distributed Representation)**\n",
    "- 분산 표현은 분포 가설(distributional hypothesis)에 기반한 방법이다.\n",
    "- 이 가설은 \"비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\"는 내용을 전제로 한다.\n",
    "- 예를 들어, '강아지'라는 단어는 '귀엽다', '예쁘다', '애교' 등의 단어와 함께 자주 등장하며, 이를 벡터화하면 해당 단어들은 유사한 벡터값을 갖게 된다.\n",
    "- 분산 표현은 단어의 의미를 여러 차원에 걸쳐 분산하여 표현한다.  \n",
    "- 이 방식은 원-핫 벡터처럼 단어 집합 크기만큼의 차원이 필요하지 않으며, 상대적으로 저차원으로 줄어든다.\n",
    "- 예를 들어, 단어 집합 크기가 10,000이고 '강아지'의 인덱스가 4라면, 원-핫 벡터는 다음과 같다:\n",
    "  \n",
    "- **강아지 = [0 0 0 0 1 0 0 ... 0]** (뒤에 9,995개의 0 포함)  \n",
    "- 그러나 Word2Vec으로 임베딩된 벡터는 단어 집합 크기와 무관하며, 설정된 차원의 수만큼 실수값을 가진 벡터가 된다:  \n",
    "- **강아지 = [0.2 0.3 0.5 0.7 0.2 ... 0.2]**  \n",
    "\n",
    "**요약하면,**\n",
    "- 희소 표현은 고차원에서 각 차원이 분리된 방식으로 단어를 표현하지만, 분산 표현은 저차원에서 단어의 의미를 여러 차원에 분산시켜 표현한다.\n",
    "- 이를 통해 단어 벡터 간 유의미한 유사도를 계산할 수 있으며, 대표적인 학습 방법으로 Word2Vec이 사용된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1aa07b",
   "metadata": {},
   "source": [
    "### Embedding Vector 시각화 wevi\n",
    "https://ronxin.github.io/wevi/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db86ffc9",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- 2013년 구글에서 개발한 Word Embedding 방법\n",
    "- 최초의 neural embedding model\n",
    "- 매우 큰 corpus에서 자동 학습\n",
    "    - 비지도 지도 학습 (자기 지도학습)이라 할 수 있음\n",
    "    - 많은 데이터를 기반으로 label 값 유추하고 이를 지도학습에 사용\n",
    "- ex) \n",
    "    - **이사금**께 충성을 맹세하였다.\n",
    "    - **왕**께 충성을 맹세하였다.\n",
    "\n",
    "**WordVec 훈련방식에 따른 구분**\n",
    "1. CBOW : 주변 단어로 중심 단어를 예측\n",
    "2. Skip-gram : 중심 단어로 주변 단어를 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad08f6",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words)  \n",
    "- CBOW는 원-핫 벡터를 사용하지만, 이는 단순히 위치를 가리킬 뿐 vocabulary를 직접적으로 참조하지 않는다.  \n",
    "\n",
    "**예시:**  \n",
    "\n",
    "> The fat cat sat on the mat  \n",
    "\n",
    "주어진 문장에서 'sat'이라는 단어를 예측하는 것이 CBOW의 주요 작업이다.  \n",
    "- **중심 단어(center word):** 예측하려는 단어 ('sat')  \n",
    "- **주변 단어(context word):** 예측에 사용되는 단어들  \n",
    "\n",
    "중심 단어를 예측하기 위해 앞뒤 몇 개의 단어를 참고할지 결정하는 범위를 **윈도우(window)**라고 한다.  \n",
    "예를 들어, 윈도우 크기가 2이고 중심 단어가 'sat'라면, 앞의 두 단어(fat, cat)와 뒤의 두 단어(on, the)를 입력으로 사용한다.  \n",
    "윈도우 크기가 n일 경우, 참고하는 주변 단어의 개수는 총 2n이다. 윈도우를 옆으로 이동하며 학습 데이터를 생성하는 방법을 **슬라이딩 윈도우(sliding window)**라고 한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG)\n",
    "\n",
    "\n",
    "**훈련 과정**\n",
    "\n",
    "CBOW는 embedding 벡터를 학습하기 위한 구조를 갖는다. 초기에는 가중치가 임의의 값으로 설정되며, 역전파를 통해 최적화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG)\n",
    "\n",
    "Word2Vec은 은닉층이 하나뿐인 얕은 신경망(shallow neural network) 구조를 사용한다.  \n",
    "학습 대상이 되는 주요 가중치는 두 가지이다:  \n",
    "\n",
    "1. **투사층(projection layer):**  \n",
    "   - 활성화 함수가 없으며 룩업 테이블 연산을 담당한다.  \n",
    "   - 입력층과 투사층 사이의 가중치 W는 V × M 행렬로 표현되며, 여기서 **V는 단어 집합의 크기, M은 벡터의 차원**이다.  \n",
    "   - W 행렬의 각 행은 학습 후 단어의 M차원 임베딩 벡터로 간주된다.  \n",
    "   - 예를 들어, 벡터 차원을 5로 설정하면 각 단어의 임베딩 벡터는 5차원이 된다.  \n",
    "\n",
    "2. **출력층:**  \n",
    "   - 투사층과 출력층 사이의 가중치 W'는 M × V 행렬로 표현된다.  \n",
    "   - 이 두 행렬(W와 W')은 서로 독립적이며, 학습 전에는 랜덤 값으로 초기화된다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_3.PNG)\n",
    "\n",
    "\n",
    "**예측 과정**\n",
    "1. CBOW는 계산된 룩업 테이블의 평균을 구한 뒤, 출력층의 가중치 W'와 내적한다.  \n",
    "2. 결과값은 **소프트맥스(softmax)** 활성화 함수에 입력되어, 중심 단어일 확률을 나타내는 예측값으로 변환된다.  \n",
    "3. 출력된 예측값(스코어 벡터)은 실제 타겟 원-핫 벡터와 비교되며, **크로스 엔트로피(cross-entropy)** 함수로 손실값을 계산한다.  \n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_5.PNG)\n",
    "\n",
    "손실 함수 식:  \n",
    "$\n",
    "cost(\\hat{y}, y) = -\\sum_{j=1}^{V} y_{j} \\cdot log(\\hat{y}_{j})\n",
    "$  \n",
    "\n",
    "여기서, $\\hat{y}_{j}$는 예측 확률, $y_{j}$는 실제 값이며, V는 단어 집합의 크기를 의미한다.  \n",
    "\n",
    "\n",
    "**학습 결과**  \n",
    "- 역전파를 통해 가중치 W와 W'가 학습된다. \n",
    "- 학습이 완료되면 W 행렬의 각 행을 단어의 임베딩 벡터로 사용하거나, W와 W' 모두를 이용해 임베딩 벡터를 생성할 수 있다.  \n",
    "- CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 구조를 갖추고 있으며, 이를 통해 단어 간 의미적 관계를 효과적으로 학습할 수 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4e41d",
   "metadata": {},
   "source": [
    "##### Skip-gram\n",
    "- Skip-gram은 중심 단어에서 주변 단어를 예측한다.\n",
    "- 윈도우 크기가 2일 때, 데이터셋은 다음과 같이 구성된다.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/skipgram_dataset.PNG)\n",
    "\n",
    "![](https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG)\n",
    "\n",
    "- 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다.\n",
    "- 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c68cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ad4dc",
   "metadata": {},
   "source": [
    "##### 영어 Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c5df1",
   "metadata": {},
   "source": [
    "- 데이터 취득 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb20cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1TF1yAHF3qRINbXWFOajFjUCxUF64QZMX\"\n",
    "output = \"ted_en.xml\"\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2320f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml 데이터 처리\n",
    "f = open('ted_en.xml', 'r', encoding='UTF-8')\n",
    "xml = etree.parse(f)\n",
    "\n",
    "contents = xml.xpath('//content/text()')    # content 태그 하위 텍스트\n",
    "# contents[:5]\n",
    "\n",
    "corpus = '\\n'.join(contents)\n",
    "print(len(corpus))\n",
    "\n",
    "# 정규식을 이용해 (Laughter), (Applause) 등 키워드 제거\n",
    "corpus = re.sub(r'\\([^)]*\\)', '', corpus)\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e8aae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'reasons', 'companies', 'fail', 'new'],\n",
       " ['real',\n",
       "  'real',\n",
       "  'solution',\n",
       "  'quality',\n",
       "  'growth',\n",
       "  'figuring',\n",
       "  'balance',\n",
       "  'two',\n",
       "  'activities',\n",
       "  'exploration',\n",
       "  'exploitation'],\n",
       " ['necessary', 'much', 'good', 'thing'],\n",
       " ['consider', 'facit'],\n",
       " ['actually', 'old', 'enough', 'remember']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 (토큰화/대소문자 정규화/불용어 처리)\n",
    "sentences = sent_tokenize(corpus)\n",
    "\n",
    "preprocessed_sentences = []\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z0-9]', ' ', sentence)  # 영소문자, 숫자 외 제거\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [token for token in tokens if token not in en_stopwords]\n",
    "    preprocessed_sentences.append(tokens)\n",
    "\n",
    "preprocessed_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803e7a7",
   "metadata": {},
   "source": [
    "- Embedding 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09800f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21462, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=preprocessed_sentences,   # corpus\n",
    "    vector_size=100,                    # 임베딩 벡터 차원\n",
    "    sg=0,                               # 학습 알고리즘 선택 (0=CBOW, 1=Skip-gram)\n",
    "    window=5,                           # 주변 단어 수 (앞뒤로 n개 고려)\n",
    "    min_count=5                         # 최소 빈도 (빈도 n개 미만은 제거)\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e612a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>-0.426447</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>-0.203567</td>\n",
       "      <td>-0.044891</td>\n",
       "      <td>0.155034</td>\n",
       "      <td>-0.758397</td>\n",
       "      <td>0.067036</td>\n",
       "      <td>0.584973</td>\n",
       "      <td>-2.454317</td>\n",
       "      <td>-0.495493</td>\n",
       "      <td>...</td>\n",
       "      <td>2.369974</td>\n",
       "      <td>0.735569</td>\n",
       "      <td>-0.427374</td>\n",
       "      <td>-0.097883</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>-1.453666</td>\n",
       "      <td>-0.703005</td>\n",
       "      <td>0.822570</td>\n",
       "      <td>0.661119</td>\n",
       "      <td>0.395817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>-2.085964</td>\n",
       "      <td>0.694396</td>\n",
       "      <td>0.511066</td>\n",
       "      <td>0.418444</td>\n",
       "      <td>0.462426</td>\n",
       "      <td>-0.988616</td>\n",
       "      <td>-0.779378</td>\n",
       "      <td>1.118007</td>\n",
       "      <td>-1.724901</td>\n",
       "      <td>-2.524780</td>\n",
       "      <td>...</td>\n",
       "      <td>1.748258</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.979481</td>\n",
       "      <td>-0.502539</td>\n",
       "      <td>0.311006</td>\n",
       "      <td>-0.020856</td>\n",
       "      <td>-0.656622</td>\n",
       "      <td>0.039620</td>\n",
       "      <td>-2.243593</td>\n",
       "      <td>0.919864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>-0.477806</td>\n",
       "      <td>-0.362631</td>\n",
       "      <td>-0.807715</td>\n",
       "      <td>-1.439073</td>\n",
       "      <td>0.253955</td>\n",
       "      <td>-0.795757</td>\n",
       "      <td>0.091464</td>\n",
       "      <td>1.315899</td>\n",
       "      <td>-0.958236</td>\n",
       "      <td>0.471730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.395872</td>\n",
       "      <td>0.682126</td>\n",
       "      <td>0.226646</td>\n",
       "      <td>-0.006821</td>\n",
       "      <td>0.611559</td>\n",
       "      <td>1.009720</td>\n",
       "      <td>0.307204</td>\n",
       "      <td>0.431559</td>\n",
       "      <td>0.870595</td>\n",
       "      <td>-0.400546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>-0.473490</td>\n",
       "      <td>-0.066556</td>\n",
       "      <td>-0.364803</td>\n",
       "      <td>0.222061</td>\n",
       "      <td>-0.132436</td>\n",
       "      <td>0.129561</td>\n",
       "      <td>-0.284361</td>\n",
       "      <td>0.162976</td>\n",
       "      <td>-0.702368</td>\n",
       "      <td>-1.105048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200937</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>-0.597488</td>\n",
       "      <td>0.149633</td>\n",
       "      <td>-0.345384</td>\n",
       "      <td>0.341046</td>\n",
       "      <td>0.484398</td>\n",
       "      <td>-0.711603</td>\n",
       "      <td>0.430580</td>\n",
       "      <td>-0.614965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>-0.779331</td>\n",
       "      <td>0.087293</td>\n",
       "      <td>-0.604736</td>\n",
       "      <td>-0.267170</td>\n",
       "      <td>0.654846</td>\n",
       "      <td>-0.254071</td>\n",
       "      <td>-0.482264</td>\n",
       "      <td>1.136506</td>\n",
       "      <td>-1.570428</td>\n",
       "      <td>-0.565255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549321</td>\n",
       "      <td>-1.215877</td>\n",
       "      <td>-0.281895</td>\n",
       "      <td>2.044843</td>\n",
       "      <td>-0.031240</td>\n",
       "      <td>0.314490</td>\n",
       "      <td>0.048706</td>\n",
       "      <td>-0.665718</td>\n",
       "      <td>-0.249015</td>\n",
       "      <td>0.430008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>-0.020931</td>\n",
       "      <td>-0.160887</td>\n",
       "      <td>0.673843</td>\n",
       "      <td>-0.614183</td>\n",
       "      <td>0.046657</td>\n",
       "      <td>-0.419444</td>\n",
       "      <td>0.358997</td>\n",
       "      <td>-0.665391</td>\n",
       "      <td>-1.378730</td>\n",
       "      <td>-1.433096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531267</td>\n",
       "      <td>1.250574</td>\n",
       "      <td>-0.672463</td>\n",
       "      <td>0.752847</td>\n",
       "      <td>0.928203</td>\n",
       "      <td>-0.404486</td>\n",
       "      <td>-0.190932</td>\n",
       "      <td>-1.096680</td>\n",
       "      <td>0.033618</td>\n",
       "      <td>-0.523750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.050989</td>\n",
       "      <td>0.341243</td>\n",
       "      <td>0.353976</td>\n",
       "      <td>-1.292722</td>\n",
       "      <td>-0.355633</td>\n",
       "      <td>-0.346536</td>\n",
       "      <td>-0.669433</td>\n",
       "      <td>0.497507</td>\n",
       "      <td>-1.907097</td>\n",
       "      <td>0.452011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324430</td>\n",
       "      <td>1.145679</td>\n",
       "      <td>-0.312920</td>\n",
       "      <td>0.881507</td>\n",
       "      <td>0.581683</td>\n",
       "      <td>0.714810</td>\n",
       "      <td>1.047979</td>\n",
       "      <td>-0.676460</td>\n",
       "      <td>0.705037</td>\n",
       "      <td>0.037364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>0.682226</td>\n",
       "      <td>-0.124611</td>\n",
       "      <td>0.877705</td>\n",
       "      <td>-0.026912</td>\n",
       "      <td>1.342556</td>\n",
       "      <td>0.508915</td>\n",
       "      <td>-0.933104</td>\n",
       "      <td>0.163822</td>\n",
       "      <td>-1.416201</td>\n",
       "      <td>-0.479037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451770</td>\n",
       "      <td>-1.525664</td>\n",
       "      <td>0.295010</td>\n",
       "      <td>2.186729</td>\n",
       "      <td>-0.082912</td>\n",
       "      <td>0.775969</td>\n",
       "      <td>-0.877580</td>\n",
       "      <td>-0.594085</td>\n",
       "      <td>-0.837899</td>\n",
       "      <td>-0.945552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>-1.365808</td>\n",
       "      <td>-0.261168</td>\n",
       "      <td>0.164824</td>\n",
       "      <td>0.794755</td>\n",
       "      <td>-0.385340</td>\n",
       "      <td>-1.120882</td>\n",
       "      <td>1.182827</td>\n",
       "      <td>0.826366</td>\n",
       "      <td>-0.595678</td>\n",
       "      <td>-1.178966</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244206</td>\n",
       "      <td>-0.134629</td>\n",
       "      <td>-0.056369</td>\n",
       "      <td>0.534370</td>\n",
       "      <td>0.920315</td>\n",
       "      <td>-0.252224</td>\n",
       "      <td>-0.426226</td>\n",
       "      <td>-1.249485</td>\n",
       "      <td>0.291435</td>\n",
       "      <td>-0.032549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>-2.311809</td>\n",
       "      <td>-1.184681</td>\n",
       "      <td>-0.952100</td>\n",
       "      <td>-0.132619</td>\n",
       "      <td>0.467133</td>\n",
       "      <td>-0.871650</td>\n",
       "      <td>-0.891329</td>\n",
       "      <td>0.342636</td>\n",
       "      <td>0.029520</td>\n",
       "      <td>-1.170334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026691</td>\n",
       "      <td>-0.558083</td>\n",
       "      <td>-0.708294</td>\n",
       "      <td>0.394697</td>\n",
       "      <td>1.423720</td>\n",
       "      <td>-0.123291</td>\n",
       "      <td>0.946036</td>\n",
       "      <td>0.085848</td>\n",
       "      <td>-0.869573</td>\n",
       "      <td>0.846294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "one    -0.426447  0.008053 -0.203567 -0.044891  0.155034 -0.758397  0.067036   \n",
       "people -2.085964  0.694396  0.511066  0.418444  0.462426 -0.988616 -0.779378   \n",
       "like   -0.477806 -0.362631 -0.807715 -1.439073  0.253955 -0.795757  0.091464   \n",
       "know   -0.473490 -0.066556 -0.364803  0.222061 -0.132436  0.129561 -0.284361   \n",
       "going  -0.779331  0.087293 -0.604736 -0.267170  0.654846 -0.254071 -0.482264   \n",
       "think  -0.020931 -0.160887  0.673843 -0.614183  0.046657 -0.419444  0.358997   \n",
       "see     0.050989  0.341243  0.353976 -1.292722 -0.355633 -0.346536 -0.669433   \n",
       "would   0.682226 -0.124611  0.877705 -0.026912  1.342556  0.508915 -0.933104   \n",
       "really -1.365808 -0.261168  0.164824  0.794755 -0.385340 -1.120882  1.182827   \n",
       "get    -2.311809 -1.184681 -0.952100 -0.132619  0.467133 -0.871650 -0.891329   \n",
       "\n",
       "              7         8         9   ...        90        91        92  \\\n",
       "one     0.584973 -2.454317 -0.495493  ...  2.369974  0.735569 -0.427374   \n",
       "people  1.118007 -1.724901 -2.524780  ...  1.748258  0.000068 -0.979481   \n",
       "like    1.315899 -0.958236  0.471730  ... -0.395872  0.682126  0.226646   \n",
       "know    0.162976 -0.702368 -1.105048  ...  0.200937  0.051417 -0.597488   \n",
       "going   1.136506 -1.570428 -0.565255  ...  0.549321 -1.215877 -0.281895   \n",
       "think  -0.665391 -1.378730 -1.433096  ...  1.531267  1.250574 -0.672463   \n",
       "see     0.497507 -1.907097  0.452011  ... -0.324430  1.145679 -0.312920   \n",
       "would   0.163822 -1.416201 -0.479037  ... -0.451770 -1.525664  0.295010   \n",
       "really  0.826366 -0.595678 -1.178966  ...  1.244206 -0.134629 -0.056369   \n",
       "get     0.342636  0.029520 -1.170334  ... -0.026691 -0.558083 -0.708294   \n",
       "\n",
       "              93        94        95        96        97        98        99  \n",
       "one    -0.097883  0.323600 -1.453666 -0.703005  0.822570  0.661119  0.395817  \n",
       "people -0.502539  0.311006 -0.020856 -0.656622  0.039620 -2.243593  0.919864  \n",
       "like   -0.006821  0.611559  1.009720  0.307204  0.431559  0.870595 -0.400546  \n",
       "know    0.149633 -0.345384  0.341046  0.484398 -0.711603  0.430580 -0.614965  \n",
       "going   2.044843 -0.031240  0.314490  0.048706 -0.665718 -0.249015  0.430008  \n",
       "think   0.752847  0.928203 -0.404486 -0.190932 -1.096680  0.033618 -0.523750  \n",
       "see     0.881507  0.581683  0.714810  1.047979 -0.676460  0.705037  0.037364  \n",
       "would   2.186729 -0.082912  0.775969 -0.877580 -0.594085 -0.837899 -0.945552  \n",
       "really  0.534370  0.920315 -0.252224 -0.426226 -1.249485  0.291435 -0.032549  \n",
       "get     0.394697  1.423720 -0.123291  0.946036  0.085848 -0.869573  0.846294  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a048ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 임베딩 모델 저장\n",
    "model.wv.save_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54b39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 로드\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "load_model = KeyedVectors.load_word2vec_format('ted_en_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80dc4d",
   "metadata": {},
   "source": [
    "- 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "559f98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.9022714495658875),\n",
       " ('daughter', 0.8022275567054749),\n",
       " ('girl', 0.798689067363739),\n",
       " ('son', 0.7826333045959473),\n",
       " ('boy', 0.7730448246002197),\n",
       " ('father', 0.7700269818305969),\n",
       " ('grandfather', 0.7668368816375732),\n",
       " ('lady', 0.7649033665657043),\n",
       " ('mother', 0.7484723925590515),\n",
       " ('sister', 0.7448771595954895)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')\n",
    "# model.wv.most_similar('abracadabra')  # 임베딩 벡터에 없는 단어로 조회 시 KeyError 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0cc5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8870137929916382),\n",
       " ('girl', 0.7925921082496643),\n",
       " ('daughter', 0.7883424758911133),\n",
       " ('lady', 0.7738549113273621),\n",
       " ('son', 0.7699402570724487),\n",
       " ('grandfather', 0.7528258562088013),\n",
       " ('father', 0.7513311505317688),\n",
       " ('boy', 0.7505763173103333),\n",
       " ('mother', 0.7311496734619141),\n",
       " ('uncle', 0.7251353859901428)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.most_similar('man')  # Word2Vec.wv = KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "221b03fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7194227"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'husband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f12c6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.2879643e-01, -3.0044821e-01,  4.2521790e-01,  1.8718352e+00,\n",
       "       -8.4118724e-01,  4.6766347e-01, -7.5917524e-01,  1.6712246e+00,\n",
       "        4.9140681e-02, -1.4736041e+00, -4.0177089e-01,  4.8642573e-01,\n",
       "       -1.1644262e-01,  3.3756867e-01,  9.5020527e-01, -2.3840456e-01,\n",
       "        4.9161723e-01, -6.3909099e-02, -5.1516598e-01, -5.0551754e-01,\n",
       "        8.5300589e-01,  8.2004225e-01,  6.2023807e-01,  1.2057844e-01,\n",
       "        5.9389967e-01,  9.8673433e-01, -1.1947743e+00, -2.7701479e-01,\n",
       "        1.0605298e-03,  4.5462483e-01, -1.2132026e+00, -1.8102828e+00,\n",
       "       -7.9829782e-02, -1.0600160e+00, -4.8994625e-01,  7.3688495e-01,\n",
       "       -5.6325179e-01, -2.0269296e-01,  8.4737957e-01,  2.0402491e-01,\n",
       "        5.9833950e-01,  4.5369023e-01,  4.7204328e-01,  1.0887465e+00,\n",
       "        2.0235262e+00,  7.2228587e-01, -5.5969357e-01,  8.8723525e-02,\n",
       "        3.2133234e-01, -1.8958358e-01,  9.5137131e-01, -6.1893636e-01,\n",
       "       -5.9657997e-01, -3.6298013e-01,  3.7589616e-01,  4.5921963e-01,\n",
       "       -1.1120051e-01, -4.2125350e-01,  6.5337852e-02, -7.9006657e-02,\n",
       "        1.3687427e-01, -1.0025580e+00, -1.5253198e+00,  6.9552577e-01,\n",
       "       -9.5870245e-01,  7.4311715e-01, -2.5296116e-01,  1.4936851e-01,\n",
       "        7.0975709e-01,  2.0485587e+00, -6.8195271e-01, -1.0056418e+00,\n",
       "        4.2093840e-01, -1.3757236e+00,  1.0822340e-01,  3.8847864e-01,\n",
       "        6.0240948e-01,  3.3621156e-01,  3.9706084e-01, -4.9496129e-01,\n",
       "       -1.1779412e+00, -9.2210233e-02, -1.0045605e+00,  9.9778646e-01,\n",
       "       -3.8375133e-01, -9.9069312e-02, -4.0254936e-01, -1.6356719e-01,\n",
       "        1.8620054e-01,  4.3824106e-01,  8.7770663e-02,  4.6806172e-02,\n",
       "        3.9319852e-01, -1.2528239e+00,  2.7465385e-01,  5.0340807e-01,\n",
       "        1.0825824e+00,  4.9215516e-01, -6.7992944e-01, -1.5871699e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6529fc",
   "metadata": {},
   "source": [
    "- 임베딩 시각화\n",
    "\n",
    "https://projector.tensorflow.org/\n",
    "\n",
    "- embedding vector(tensor) 파일 (.tsv)\n",
    "- metadata 파일 (.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3da9568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 16:14:25,711 - word2vec2tensor - INFO - running c:\\Users\\Playdata\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input ted_en_w2v --output ted_en_w2v\n",
      "2025-04-07 16:14:25,712 - keyedvectors - INFO - loading projection weights from ted_en_w2v\n",
      "2025-04-07 16:14:26,708 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (21462, 100) matrix of type float32 from ted_en_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T16:14:26.605335', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 16:14:27,438 - word2vec2tensor - INFO - 2D tensor file saved to ted_en_w2v_tensor.tsv\n",
      "2025-04-07 16:14:27,438 - word2vec2tensor - INFO - Tensor metadata file saved to ted_en_w2v_metadata.tsv\n",
      "2025-04-07 16:14:27,439 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input ted_en_w2v --output ted_en_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28761a9e",
   "metadata": {},
   "source": [
    "##### 한국어 Word Embedding\n",
    "- NSMC (Naver Sentiment Movie Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef4dfa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77d22608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naver_movie_ratings.txt', <http.client.HTTPMessage at 0x20a57057500>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"naver_movie_ratings.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d5b1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 생성\n",
    "ratings_df = pd.read_csv('naver_movie_ratings.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a031c117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    8\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 결측치 확인 및 처리(제거)\n",
    "display(ratings_df.isnull().sum())\n",
    "\n",
    "ratings_df = ratings_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "137c193a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200    많은 생각을 할 수 있는 영화~ 시간여행류의 스토리를 좋아하는 사람이라면 빠트릴 수...\n",
       "201    고소한 19 정말 재미있게 잘 보고 있습니다^^ 방송만 보면 털털하고 인간적이신 것...\n",
       "202                                                  가연세\n",
       "203                         goodgoodgoodgoodgoodgoodgood\n",
       "204                                           이물감. 시 같았다\n",
       "                             ...                        \n",
       "295                                   박력넘치는 스턴트 액션 평작이다!\n",
       "296                                      엄청 재미있다 명작이다 ~~\n",
       "297    나는 하정우랑 개그코드가 맞나보다 엄청 재밌게봤네요 특히 단발의사샘 장면에서 계속 ...\n",
       "298                                                적당 ㅎㅎ\n",
       "299                                    배경이 이쁘고 캐릭터도 귀엽네~\n",
       "Name: document, Length: 100, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df['document'][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a5c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글이 아닌 데이터 제거\n",
    "ratings_df['document'] = ratings_df['document'].replace(r'[^0-9가-힣ㄱ-ㅎㅏ-ㅣ\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b09d2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199992/199992 [06:11<00:00, 538.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# 전처리\n",
    "from tqdm import tqdm   # 진행도 시각화\n",
    "\n",
    "okt = Okt()\n",
    "ko_stopwords = ['은', '는', '이', '가', '을', '를', '와', '과', '들', '도', '부터', '까지', '에', '나', '너', '그', '걔', '얘']\n",
    "\n",
    "\n",
    "preprocessed_data = []\n",
    "\n",
    "for sentence in tqdm(ratings_df['document']):\n",
    "    tokens = okt.morphs(sentence, stem=True)\n",
    "    tokens = [token for token in tokens if token not in ko_stopwords]\n",
    "    preprocessed_data.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "658a56b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16841, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=preprocessed_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0    # CBOW\n",
    ")\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5375eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화관', 0.9426742196083069),\n",
       " ('틀어주다', 0.8109181523323059),\n",
       " ('학교', 0.7896940112113953),\n",
       " ('케이블', 0.7860040664672852),\n",
       " ('메가박스', 0.7162373065948486),\n",
       " ('티비', 0.7134400606155396),\n",
       " ('방금', 0.6961232423782349),\n",
       " ('영화제', 0.6838327646255493),\n",
       " ('개봉관', 0.6705398559570312),\n",
       " ('투니버스', 0.6653599739074707)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('극장')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53fcd118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76677406"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('김혜수', '전지현')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea022519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.wv.save_word2vec_format('naver_movie_ratings_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69660806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:11:10,888 - word2vec2tensor - INFO - running c:\\Users\\Playdata\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v\n",
      "2025-04-07 17:11:10,888 - keyedvectors - INFO - loading projection weights from naver_movie_ratings_w2v\n",
      "2025-04-07 17:11:11,715 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (16841, 100) matrix of type float32 from naver_movie_ratings_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2025-04-07T17:11:11.614989', 'gensim': '4.3.3', 'python': '3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'load_word2vec_format'}\n",
      "2025-04-07 17:11:12,258 - word2vec2tensor - INFO - 2D tensor file saved to naver_movie_ratings_w2v_tensor.tsv\n",
      "2025-04-07 17:11:12,258 - word2vec2tensor - INFO - Tensor metadata file saved to naver_movie_ratings_w2v_metadata.tsv\n",
      "2025-04-07 17:11:12,259 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input naver_movie_ratings_w2v --output naver_movie_ratings_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443b27e",
   "metadata": {},
   "source": [
    "- 사전 훈련된 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8739a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\n",
      "From (redirected): https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c&confirm=t&uuid=30c1a16c-aba0-491b-80bd-ee02d34f8ec4\n",
      "To: c:\\encore_skn11\\07_nlp\\03_word_embedding\\GoogleNews_vecs.bins.gz\n",
      "100%|██████████| 1.65G/1.65G [06:25<00:00, 4.27MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GoogleNews_vecs.bins.gz'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?id=1aL_xpWW-CjfCrLWeflIaipITOZ6zHI5c\"\n",
    "output = \"GoogleNews_vecs.bins.gz\"\n",
    "\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e81f8ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv = KeyedVectors.load_word2vec_format('GoogleNews_vecs.bins.gz', binary=True)\n",
    "google_news_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0ffd99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22942671"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similarity('king', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "747cbdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138044834136963),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159994602203369)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.most_similar('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5baa953f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24791394"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.n_similarity(['king', 'queen'], ['man', 'woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e38df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138044834136963),\n",
       " ('queen', 0.6510957479476929),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159994602203369)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.similar_by_word('king', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5b019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_wv.has_index_for('ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
